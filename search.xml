<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>Exact Methods for Chain-Structured Models</title>
      <link href="/2022/05/10/monte-carlo-2-4/"/>
      <url>/2022/05/10/monte-carlo-2-4/</url>
      
        <content type="html"><![CDATA[<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>Here we discuss an important probability distribution used in many applications which has the following form:</p><script type="math/tex; mode=display">\pi(\mathbf{x})\propto \exp\{-\sum_{i=1}^d h_i (x_{i-1}, x_i)\},</script><p>where $\mathbf{x}=(x_0, x_1,\dots,x_d)$.Clearly, this type of model has a “Markovian structure” as the conditional distribution of $x_i$ depends only on its two neighboring variables $x_{i-1}$ and $x_{i+1}$. When, $x_0,…,x_d$ are discrete random variables taking values in a finite set $\mathcal{S}=\{s_1, …,s_k\}$, this structure is often referred to as the <em>hidden Markov model</em> (HMM).<br>We introduce two methods to deal with a hidden Markov model:</p><ul><li>Dynamic programming, to find the global maximum of $\pi(\mathbf{x})$ and its maximizer $\hat{\mathbf{x}}$;</li><li>Propagation method, to find the marginal distribution of each $x_i$ and drawing samples from $\pi(x)$.<h2 id="Mehtods"><a href="#Mehtods" class="headerlink" title="Mehtods"></a>Mehtods</h2><h3 id="Dynamic-Programming-Bellman-1957"><a href="#Dynamic-Programming-Bellman-1957" class="headerlink" title="Dynamic Programming [Bellman 1957]"></a>Dynamic Programming [<em>Bellman 1957</em>]</h3>Dynamic programming is a recursive algorithm used for finding both the global maximum and its maximizer for $\pi(\mathbf{x})$. It makes use of the chain structure and solve this problem by updating the minimum function for each $x_i$ iteratively. Briefly summarize here. Note that all notations are the same as they in Section <strong>Introduction</strong>.<br>Suppose we want to maximize $\pi(\mathbf{x})$, it is equivalent to minimizing its exponent $H(\mathbf{x})=\sum_{i=1}^{d}h_i(x_{i-1}, x_i)$.<br>The dynamic programming method works as following:</li><li>Define $m_1(x) = \min_{s_i\in \mathcal{S}} h_1(s_i, x)$, for $x\in \mathcal{S}$.</li><li>Recursively compute $m_t(x)=\min_{s_i\in\mathcal{S}}\{m_{t-1}(s_i)+h_t(s_i,x)\}$, for $x\in \mathcal{S}$.</li><li>Find the optimal value $H(x)$ by computing $\min_{s_i\in\mathcal{S}} m_d(s)$.<br>Through simple induction, one can check that:<script type="math/tex; mode=display">\min_{x\in\mathcal{S}}m_t(x) = \min_{x_i\in\mathcal{S}}\sum_{i=1}^d h_i(x_{i-1}, x_i).</script>To find out which $\mathbf{x}$ gives rise to the global minimum of $H(\mathbf{x})$, we can trace backward as follows:<br>1.Let $\hat{x}_d$ be the minimizer of $m_d(x)$, which is $\hat{x}_d =\mathrm{arg}\min_{s_i\in\mathcal{S}}m_d(s_i)$;<br>2.For $t=d-1,d-2,…,1$, we let<script type="math/tex; mode=display">\hat{x}_t = \mathrm{arg}\min_{s_i\in\mathcal{S}} \min\{m_t(s_i)+h_{t+1}(s_i, \hat{x}_{t+1})\}</script>Then the configuration $\hat{\mathbf{x}}=(\hat{x}_1,…,\hat{x}_d)$ is the minimizer of $H(\mathbf{x})$.<h3 id="Propagation-Method"><a href="#Propagation-Method" class="headerlink" title="Propagation Method"></a>Propagation Method</h3>To simulate from $\pi(\mathbf{x})$, we need to draw $x_d$ from its marginal distribution firstly, which requires us to marginalize $x_1,…,x_{d-1}$ in the joint distribution $\pi(\mathbf{x})$. After we have drawn $x_d$, we can work our way backward recursively to sample $x_{d-1},…,x_0$. The marginalization step is based on the observation that the overall summation can be decomposed into recursive steps, which is<script type="math/tex; mode=display">Z =\sum_x\exp(-H(x))=\sum_{x_d}[\cdots[\sum_{x_1}\{\sum_{x_0}\exp(-h_1(x_0,x_1))\}\exp(-h_2(x_1, x_2))]\cdots].</script>The detailed procedures for computing the partition function is:</li><li>Define $V_1(x)=\sum_{x_0\in\mathcal{S}}\exp(-h_1(x_0,x))$;</li><li>Compute recursively for $t=2,\dots,d$, $V_t(x)=\sum_{y\in\mathcal{S}}V_{t-1}(y)\exp(-h_t(y,x))$;</li><li>Compute the partition function $Z=\sum_{x\in\mathcal{S}}V_d(x)$.<br>With the partition function $Z$, we can sample from $\pi(x)$ as following:</li><li>Draw $x_d$ from $\mathcal{S}$ with probability $V_d(x_d)/Z$;</li><li>For $t=d-1,\dots,1$, draw $x_t$ from<script type="math/tex; mode=display">p_t(x)= \frac{V_t(x)\exp(-h_{t+1}(x,x_{t+1}))}{\sum_{y\in S}V_t(y)\exp(-h_{t+1}(y,x_{t+1}))}</script>Then  random sample $\mathbf{x}=(x_1,\dots,x_d)$ obtained in this way follows the distribution $\pi(\mathbf{x})$.<h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2>We use aboves methods for solving a one-dimensional Ising model:<script type="math/tex; mode=display">\pi(\mathbf{x}) = Z^{-1}\exp\{\beta(x_0 x_1 +\cdots + x_{d-1}x_d)\},</script>where $x_i$ takes values in $\mathcal{S}={-1, 1}$.<br>Oberviously, when $\beta&gt;0$, $\mathrm{arg}\max\{\pi(\mathbf{x})\}=(-1, -1,\dots,-1)$ or $(1,1,\dots,1)$, and when $\beta&lt;0$, $\mathrm{arg}\max\{\pi(\mathbf{x})\}=(-1, 1,-1,\dots)$ or $(1, -1, 1,\dots)$.<br>We first display the <code>Python</code> code for above methods as:<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">ISingModel</span>(<span class="params">beta=<span class="number">0.1</span></span>):</span><br><span class="line">    S = np.array([-<span class="number">1</span>, <span class="number">1</span>])</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">h</span>(<span class="params">l, r</span>):</span><br><span class="line">        <span class="keyword">return</span> -beta*l*r</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">compute</span>(<span class="params">x</span>):</span><br><span class="line">        ans = <span class="number">0.</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="built_in">len</span>(x)):</span><br><span class="line">            ans += h(x[i-<span class="number">1</span>], x[i])</span><br><span class="line">        <span class="keyword">return</span> ans</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">dynamic_programming</span>(<span class="params">k=<span class="number">10</span></span>):</span><br><span class="line">        m = []</span><br><span class="line">        x = []</span><br><span class="line">        <span class="comment"># forward</span></span><br><span class="line">        <span class="keyword">for</span> s <span class="keyword">in</span> S:</span><br><span class="line">            x += [np.<span class="built_in">min</span>(h(S, s))] </span><br><span class="line">        m += [np.array(x)]</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, k-<span class="number">1</span>):</span><br><span class="line">            x = []</span><br><span class="line">            <span class="keyword">for</span> s <span class="keyword">in</span> S:</span><br><span class="line">                x += [np.<span class="built_in">min</span>(m[i-<span class="number">1</span>]+h(s, S))]</span><br><span class="line">            m += [np.array(x)]</span><br><span class="line">        <span class="comment"># backward</span></span><br><span class="line">        xs = np.zeros(k)</span><br><span class="line">        xs[-<span class="number">1</span>] = S[np.argmin(m[-<span class="number">1</span>])]</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(k-<span class="number">2</span>, -<span class="number">1</span>, -<span class="number">1</span>):</span><br><span class="line">            xs[i] = S[np.argmin(m[i]+h(S, xs[i+<span class="number">1</span>]))]</span><br><span class="line">        <span class="keyword">return</span> m, xs</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">simulation</span>(<span class="params">k=<span class="number">10</span></span>):</span><br><span class="line">        V = []</span><br><span class="line">        v1 = []</span><br><span class="line">        <span class="keyword">for</span> s <span class="keyword">in</span> S:</span><br><span class="line">            v1 += [np.<span class="built_in">sum</span>(np.exp(-<span class="number">1.</span>*h(s, S)))]</span><br><span class="line">        V += [v1]</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, k):</span><br><span class="line">            v = []</span><br><span class="line">            <span class="keyword">for</span> s <span class="keyword">in</span> S:</span><br><span class="line">                v += [np.<span class="built_in">sum</span>(V[i-<span class="number">1</span>]*np.exp(-<span class="number">1.</span>*h(S, s)))]</span><br><span class="line">            V += [v]</span><br><span class="line">        x = np.zeros(k)</span><br><span class="line">        Z = np.<span class="built_in">sum</span>(V[-<span class="number">1</span>])</span><br><span class="line">        x[k-<span class="number">1</span>] = S[np.random.binomial(<span class="number">1</span>, V[k-<span class="number">1</span>][<span class="number">0</span>]/Z, <span class="number">1</span>)]</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(k-<span class="number">2</span>, -<span class="number">1</span>, -<span class="number">1</span>):</span><br><span class="line">            z = np.<span class="built_in">sum</span>(V[i]*np.exp(-<span class="number">1.</span>*h(S, x[i+<span class="number">1</span>])))</span><br><span class="line">            p1 = V[i][<span class="number">1</span>]*np.exp(-<span class="number">1.</span>*h(<span class="number">1.</span>, x[i+<span class="number">1</span>]))/z</span><br><span class="line">            x[i] = S[np.random.binomial(<span class="number">1</span>, p1, <span class="number">1</span>)]</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line">    <span class="keyword">return</span> dynamic_programming, simulation</span><br></pre></td></tr></table></figure>It is easy to verify that the dynamic programming function works well.<br>For simulating, we define the number of flips as a statistic $T$. Obviously, larger $\beta&gt;0$ will lead to smaller $T$, while smaller $\beta&lt;0$ will lead to larger $T$.<br>We fix the number of particles to be $10$, and here is our result:<br><img src="https://raw.githubusercontent.com/ZechangSun/image-hosting/main/image/number-of-flip.1ext5lvghyo0.webp" alt="number-of-clips"><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2></li><li><em>Monte Carlo Strategies in Scientific Computing</em>, Jun S. Liu, 2001</li></ul>]]></content>
      
      
      <categories>
          
          <category> Reading Notes </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Monte Carlo Strategies in Scientific Computing </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Variance Reduction Methods</title>
      <link href="/2022/05/05/variance-reduction-methods/"/>
      <url>/2022/05/05/variance-reduction-methods/</url>
      
        <content type="html"><![CDATA[<p><em>There is no free lunch!</em></p><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>Here we briefly describe four techniques commonly used for variance reduction in Monte Carlo computations, including:</p><ul><li>Stratified Sampling</li><li>Control Variates Method</li><li>Antithetic Variates Method</li><li>Rao-Blackwellization</li></ul><h2 id="Methods"><a href="#Methods" class="headerlink" title="Methods"></a>Methods</h2><h3 id="Stratified-Sampling"><a href="#Stratified-Sampling" class="headerlink" title="Stratified Sampling"></a>Stratified Sampling</h3><p>Mathematically, this method can be viewed as a special importance sampling method with its trial density constructed as a piecewise constant function.<br>Suppose we are interested in estimating $\int_{\chi} f(x)\mathrm{d} x$. If possible, we can break the region $\chi$ into the union of $k$ disjoint subregions, $D_1$, …, $D_k$. We hope within each subregion, the function $f(x)$ is relatively “homogeneous”. Then, we can spend $m_i$ random samples, $X^{(i, 1)}$,…,$X^{(i, m_i)}$, in the subregion $D_i$, and approximate the subregion integral $\int_{D_i} f(x)\mathrm{d}x$ by:</p><script type="math/tex; mode=display">\hat{\mu}_i = \frac{1}{m_i}\sum_{j=1}^{m_i}f(X^{(i, j)}).</script><p>The overall integral $\mu$ can be approximated by</p><script type="math/tex; mode=display">\hat{\mu} = \sum_{i=1}^k\hat{\mu}_i,</script><p>whose variance can be easily calculated as:</p><script type="math/tex; mode=display">\mathrm{var}(\hat{\mu}) = \sum_{i=1}^k\frac{\sigma^2_i}{m_i},</script><p>where $\sigma^2_i$ is the variation of $f(x)$ in region $D_i$.<br>Clearly, the key is to make $f(x)$ as homogeneous as possible in each $D_i$, otherwise, stratified sampling cannot make<br>the computation more accurate than a plain Monte Carlo.<br>The moral is: one needs to think carefully before adopting any advanced techniques.</p><h3 id="Control-Variates-Method"><a href="#Control-Variates-Method" class="headerlink" title="Control Variates Method"></a>Control Variates Method</h3><p>The key of control variates method is to make use of a control variate $C$, which is correlated with the sample $X$, to produce a better estimate.<br>Suppose the estimation of $\mu=\mathbb{E}(X)$ is of interest and $\mu_C=\mathbb{E}(C)$ is known. Then, we can construct Monte Carlo samples of the form</p><script type="math/tex; mode=display">X(b) = X - b(C-\mu_C),</script><p>which have the same mean as $X$, but a new variance</p><script type="math/tex; mode=display">\mathrm{var}\{X(b)\} = \mathrm{var}(X)-2b\mathrm{cov}(X, C)+b^2\mathrm{var}(C).</script><p>If the computation of $\mathrm{cov}(X, C)$ and $\mathrm{var}(C)$ is easy, then we can let $b=\mathrm{cov}(X, C)/\mathrm{var}(C)$, in which case,</p><script type="math/tex; mode=display">\mathrm{var}\{X(b)\} = (1-\rho^2_{XC})\mathrm{var}(X)<\mathrm{var}(X).</script><h3 id="Antithetic-Variates-Method"><a href="#Antithetic-Variates-Method" class="headerlink" title="Antithetic Variates Method"></a>Antithetic Variates Method</h3><p>This method is due to Hammersley and Morton (1956), where they describe a way of producing negatively correlated samples. Suppose $U$ is the random number used in the production of a sample $X$ that follows a distribution with cdf $F$ (see <a href="https://zechangsun.github.io/2022/04/05/How-to-generate-simple-random-variables/">How to generate simple random variables?</a>). Then, $X’=F^{-1}(1-U)$ also follows distribution $F$.<br>Generally, if $g$ is a monotonic function, then</p><script type="math/tex; mode=display">\{g(u_1)-g(u_2)\}\{g(1-u_1)-g(1-u_2)\}\leq 0,</script><p>for any $u_1, u_2\in [0, 1]$. For two independent random variables $U_1$ and $U_2$ with symmetric density in $[0, 1]$, we have</p><script type="math/tex; mode=display">\mathbb{E}[\{g(U_1)-g(U_2)\}\{g(1-U_1)-g(1-U_2)\}]=\mathrm{cov}(X,X')\leq 0,</script><p>where $X=g(U)$ and $X’=g(1-U)$. Thus, $\mathrm{var}[(X+X’)/2]\leq\mathrm{var}(X)/2$, implying that using the pair $X$  and $X’$ is better than using two independent Monte Carlo draws for estimating $\mathbb{E}(X)$.</p><h3 id="Rao-Blackwellization"><a href="#Rao-Blackwellization" class="headerlink" title="Rao-Blackwellization"></a>Rao-Blackwellization</h3><p>This method reflects a basic principle in Monte Carlo computation: <strong>One should carry out analytical computation as much as possible</strong>. The problem can be formulated as follows: Suppose we have drawn independent samples $X^{(1)},…,X^{(m)}$ from the target distribution $\pi(X)$ and intetersted in evaluating $I=\mathbb{E}_{\pi} h(X)$. A straightforward estimator is</p><script type="math/tex; mode=display">\hat{I} = \frac{1}{m}\sum_{i=1}^{m}h(X^{(i)}).</script><p>Suppose, in addition, that $X$ can be decomposed into two parts $(X_1, X_2)$ and that the conditional expection $\mathbb{E}[h(X)|X_2]$ can be carried out analytically. An alternative estimator of $I$ is</p><script type="math/tex; mode=display">\tilde{I} = \frac{1}{m}\sum_{i=1}^{m}\mathbb{E}[h(X)|X_2^{(i)}].</script><p>Both $\hat{I}$ and $\hat{I}$ are unbiased as:</p><script type="math/tex; mode=display">\mathbb{E}_{\pi} h(X) = \mathbb{E}_{\pi}[\mathbb{E}\{h(X)|X_2\}].</script><p>If the computational effort for obtaining the two estimates are the same, then $\tilde{I}$ should be preferred because</p><script type="math/tex; mode=display">\mathrm{var}\{h(X)\}=\mathrm{var}\{\mathbb{E}[h(X)|x_2]\} + \mathbb{E}\{\mathrm{var}[h(X)|x_2]\},</script><p>which implies that</p><script type="math/tex; mode=display">\mathrm{var}(\hat{I}) = \frac{\mathrm{var}\{h(x)\}}{m}\geq \frac{\mathrm{var}\{\mathbb{E}[h(X)|X_2]\}}{m}=\mathrm{var}(\tilde{I}).</script><p>In statistics, $\hat{I}$ is called the “histogram estimator” and $\tilde{I}$ is the “mixture estimator”. Statisticians find that by conditioning an inferior estimator on the value of sufficient statistics, one can obtain the optimal estimator, which is referred to as <em>Rao-Blackwellization</em> (Bickel and Doksum 2000).</p><h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><h3 id="Stratified-Sampling-1"><a href="#Stratified-Sampling-1" class="headerlink" title="Stratified Sampling"></a>Stratified Sampling</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="comment"># Stratified Sampling to compute Pr(0&lt;X&lt;3), X\sim exp(1.)</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">ImportanceSampling</span>(<span class="params">N=<span class="number">300</span></span>):</span><br><span class="line">    u = np.random.uniform(<span class="number">0</span>, <span class="number">3</span>, N)</span><br><span class="line">    x = <span class="number">3.</span>*np.exp(-u)</span><br><span class="line">    <span class="keyword">return</span> np.mean(x)</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">StratifiedSampling</span>(<span class="params">N=<span class="number">300</span></span>):</span><br><span class="line">    n = <span class="built_in">int</span>(N/<span class="number">3</span>)</span><br><span class="line">    cut = [<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]</span><br><span class="line">    mu = <span class="number">0.</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">3</span>):</span><br><span class="line">        u = np.random.uniform(cut[i], cut[i+<span class="number">1</span>], n)</span><br><span class="line">        mu += np.mean(np.exp(-u))</span><br><span class="line">    <span class="keyword">return</span> mu</span><br><span class="line"><span class="comment"># draw 1000 samples:</span></span><br><span class="line">N = <span class="number">1000</span></span><br><span class="line">importance_sampling_estimator = [ImportanceSampling() <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(N)]</span><br><span class="line">stratified_sampling_estimator = [StratifiedSampling() <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(N)]</span><br><span class="line">importance_sampling_variance = np.var(importance_sampling_estimator)</span><br><span class="line">stratified_sampling_variance = np.var(stratified_sampling_estimator)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;variance for importance sampling: <span class="subst">&#123;importance_sampling_variance&#125;</span>\nvariance for stratified sampling: <span class="subst">&#123;stratified_sampling_variance&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure><p>The result is:<br>variance for importance sampling: 0.0019599072651563763<br>variance for stratified sampling: 0.00036503846713179785</p><h3 id="Control-Variates-Method-1"><a href="#Control-Variates-Method-1" class="headerlink" title="Control Variates Method"></a>Control Variates Method</h3><p>Here is a simple example: $Y\sim \mathcal{N}(0, 1)$ and $Z\sim \mathcal{N}(0, 1)$. $Y$ and $Z$ are independent. Then for $X=Y+Z$, $X(b)=X-Z$ can achieve smaller variance compared to $X$ while remain same mean.</p><h3 id="Antithetic-Variates-Method-1"><a href="#Antithetic-Variates-Method-1" class="headerlink" title="Antithetic Variates Method"></a>Antithetic Variates Method</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># estimate the mean of X\sim exp(1)</span></span><br><span class="line">u = np.random.uniform(<span class="number">0</span>, <span class="number">1</span>, <span class="number">10000</span>)</span><br><span class="line">x = -np.log(<span class="number">1</span>-u)</span><br><span class="line">x_ = -np.log(u)</span><br><span class="line">xx = (x+x_)/<span class="number">2</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;mean(x): <span class="subst">&#123;np.mean(x)&#125;</span>\nvar(x): <span class="subst">&#123;np.var(x)&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;mean((x+x&#x27;)/2): <span class="subst">&#123;np.mean(xx)&#125;</span>\nvar((x+x&#x27;)/2): <span class="subst">&#123;np.var(xx)&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure><p>The result is:<br>mean(x): 1.0054171546154012<br>var(x): 0.9831275984756698<br>mean((x+x’)/2): 0.9973795991567435<br>var((x+x’)/2): 0.16888131503823542</p><h3 id="Rao-Blackwellization-1"><a href="#Rao-Blackwellization-1" class="headerlink" title="Rao-Blackwellization"></a>Rao-Blackwellization</h3><p>Same as in control variates method, where conditional mean is zero.</p><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul><li><em>Monte Carlo Strategies in Scientific Computing</em>, Jun S. Liu, 2001</li></ul>]]></content>
      
      
      <categories>
          
          <category> Reading Notes </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Monte Carlo Strategies in Scientific Computing </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Systematic evaluation of variability detection methods for eROSITA</title>
      <link href="/2022/04/13/GSoC-paper-reading-1/"/>
      <url>/2022/04/13/GSoC-paper-reading-1/</url>
      
        <content type="html"><![CDATA[<p>This is a reading notes for a recent paper, <a href="https://arxiv.org/pdf/2106.14529.pdf"><em>Systematic evaluation of variability detection methods for eROSITA</em></a>, discussing about statistical methods for variability detection of X-ray light curves.</p><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><h3 id="What-problem-this-work-wants-to-solve"><a href="#What-problem-this-work-wants-to-solve" class="headerlink" title="What problem this work wants to solve?"></a>What problem this work wants to solve?</h3><p>This paper wants to take a closer look at <strong>how to indentify source variability in the X-rays</strong>. Compared to large-scale optical photometric surveys (Gaia, Zwicky Transient Factory, etc), in which <strong>systematics</strong> typically dominate measurement uncertainties, this paper focuses on source variability identification in repeated X-ray surveys where most X-ray sources are found near the detection limit, and <strong>statistical (Poisson) uncertainties</strong> are dominant.</p><h3 id="Why-this-problem-is-important"><a href="#Why-this-problem-is-important" class="headerlink" title="Why this problem is important?"></a>Why this problem is important?</h3><p>The variability of astrophysical sources is a powerful diagnostic to differentiate between different physical models even when those models predict similar spectral energy distribution. Variability studies have greatly enriched the zoo of astrophysical phenomena with new mysteries, e.g. fast radio bursts, ultra-luminous X-ray sources and quasi-periodic eruptions. And, in high-energy astrophysics, the search for transient phenomena has a long history with gamma ray bursts.</p><p>Apparently, a basic task in variability studies is to indentify source variability, which is a non-trival problem in the X-rays. Previous missions like MAXI, Rossi X-ray Timing Explorer and Swift are only sensitive to the brightest objects (typically fewer than one hundred variability triggers per year). However, <em>eROSITA</em> can scans the X-ray sky rapidly over large areas down to faint fluxes, so it has the potential to reveal a myriad of diverse variable and transient phenomena.</p><p>To fully exploit the <em>eROSITA</em> dataset, robust and well characterized techniques are necessary to identify, classify and characterize the variability properties of each detected X-ray source.</p><h2 id="Source-Count"><a href="#Source-Count" class="headerlink" title="Source Count"></a>Source Count</h2><h3 id="What-are-source-conunts"><a href="#What-are-source-conunts" class="headerlink" title="What are source conunts?"></a>What are source conunts?</h3><p>The first task for source variability identification is to model the counts observed in a time bin $t$, which is usually described with a Poisson process.<br>For the background region, by assuming the damped efficiency $f_{\mathrm{expo}}$ is constant within the time bin $\Delta t$, one can derive the observed background counts $B$ follows:</p><script type="math/tex; mode=display">B\sim \mathrm{Poisson}(R_B\times f_\mathrm{expo}\times \Delta t)</script><p>where $R_B$ is the background rate.<br>Further, the total counts in the source region, $S$, contain contributions from the source, with count rate $R_S$ and background can be written as:</p><script type="math/tex; mode=display">S\sim Poisson((R_S+R_B\times r)\times f_\mathrm{expo}\times \Delta t)</script><p>In above equation, the backgorund rate $R_B$ is scaled by the area ratio of source and background extraction regions, $r$, with typical values $\sim 1\%$. The unknowns are the background count rate, $R_B$ and the net source count rate, $R_S$. Typical values for $R_S/(R_B\times r)$ are $\sim 8$ for the soft band and $\sim 1$ for the hard band.</p><h3 id="How-to-estimate-source-rate"><a href="#How-to-estimate-source-rate" class="headerlink" title="How to estimate source rate?"></a>How to estimate source rate?</h3><h4 id="Classic-per-bin-source-rate-estimates"><a href="#Classic-per-bin-source-rate-estimates" class="headerlink" title="Classic per-bin source rate estimates"></a>Classic per-bin source rate estimates</h4><p>The classic point estimator for the net source count rate $R_S$ is :</p><script type="math/tex; mode=display">\hat{R}_S = \frac{S-B\times r}{f_{\mathrm{expo}}\times \Delta t}</script><p>Similarly, the background count rate in the background region can be estimated with:</p><script type="math/tex; mode=display">\hat{R}_B = B/(f_{expo}\times \Delta t)</script><p>The uncertainty in the net source count rate $\hat{R}_S(t)$ is estimated as:</p><script type="math/tex; mode=display">\hat{\sigma}(R_S)(t) = \frac{\sqrt{\hat{\sigma}(S)(t)^2 + \hat{\sigma}(B)(t)^2\times r}}{f_\mathrm{expo}(t)\times \Delta t}</script><p>in which, $\hat{\sigma}(C)$ (with $C$ either $S$ or $B$) is the uncertainty of the expected number of counts, given observed counts $C$. The choice of $\hat{\sigma}(C)$ is quite tricky, in this paper, they set $\hat{\sigma}(C)=\sqrt{C+0.75}+1$, which tends to enlarge the error bars. For more detail discussion, please refer to <a href="https://arxiv.org/pdf/2106.14529.pdf">the paper</a>.</p><h4 id="Bayesian-per-bin-source-rate-estimates"><a href="#Bayesian-per-bin-source-rate-estimates" class="headerlink" title="Bayesian per-bin source rate estimates"></a>Bayesian per-bin source rate estimates</h4><p>Above estimators face drawback of the Gaussianity error propagation. In the low-count regime, the Poisson uncertainties become asymmetric, we can then use Bayesian approach to propagate the uncertainties. We briefly summarize how to do this below.</p><p>First, for the unknown background count rate $R_B(t)$, by assigning a flat, improper prior on the expected count rate $R_B\times f_{\mathrm{expo}}$, we can find its posterior distribution can be numerically inverted using the inverse incomplete Gamma function $\Gamma^{-1}$, we can then use it to evaluate the posterior error bar of $R_B$. However, since $R_S$ depends on $S$ and $R_B(t)$, one can not simply repeat above precedures. We must first compute the marginalised likelihood function of $R_S$ over $R_B$ numerically. With this likelihood function, by assigning appropriate prior, we can compute the posterior distribution of $R_S$.</p><h2 id="Methods-for-variability-detection"><a href="#Methods-for-variability-detection" class="headerlink" title="Methods for variability detection"></a>Methods for variability detection</h2><h3 id="Amplitude-maximum-deviation-methods"><a href="#Amplitude-maximum-deviation-methods" class="headerlink" title="Amplitude maximum deviation methods"></a>Amplitude maximum deviation methods</h3><p>The simplest definition of variability is that two measured source rates disagree with each other. By assuming Gaussian error propagation, one can defined a statistic, Amplitude Maximum Deviation as the tension between the most extreme points, as:</p><script type="math/tex; mode=display">AMPL_MAX = (\hat{R}_S(t_{\mathrm{max}})-\hat{\sigma}(R_S)(t_{\mathrm{max}}))-(\hat{R}_S(t_{\mathrm{min}})+\hat{\sigma}(R_S)(t_{\mathrm{max}}))</script><p>We can find that the AMPL_MAX decribes the distance between the lower error bar of the maximum value to the upper error bar of the minimum value. And we can normalize it using the standard deviations:</p><script type="math/tex; mode=display">AMPL_SIG = \frac{AMPL_MAX}{\sqrt{\hat{\sigma}(R_S)(t_{\mathrm{max}})^2+\hat{\sigma}(R_S)(t_\mathrm{min})^2}}</script><p>This method is conservative and a main drawback of this method is the assumption that the errors are Gaussian. Also, since only the two most extreme data points are considered, it is insensitive to the variations in other values.</p><h3 id="Bayesian-blocks"><a href="#Bayesian-blocks" class="headerlink" title="Bayesian blocks"></a>Bayesian blocks</h3><p>The Bayesian blocks algorithm identifies in a sequence of measurement points where the rate changed. This adaptive binning technique automatically segments a light curve into blocks of constant rates separated by change points. Bayesian blocks begins with the hypothesis that the count rate is constant. For each candidate change point, it tries the hypothesis that the count rate is constant to some value before the change point, and constant to some value after the change point. The two hypothesis probabilities are compared using Bayesian model comparison. If the model comparison favours the split, each segment is analysed with the same procedure recursively. Finally, Bayesian blocks returns a segmented light curve, and estimates for the count rate in each segment with its uncertainties.</p><p>The main drawback of this method is that the background variation are not taken into consideration. An extension of Bayesian blocks to analyse source and background region light curve simultaneously would be desirable, building on the foundations outlined above.</p><h3 id="Fractional-and-excess-variance"><a href="#Fractional-and-excess-variance" class="headerlink" title="Fractional and excess variance"></a>Fractional and excess variance</h3><p>The idea of eccess variance methods is to examine whether the observed stochasticity shows addtional variance, i.e., is over-dispersed. </p><p>Formally, across bins, the mean net source count rate $\bar{R}_S$ is:</p><script type="math/tex; mode=display">\bar{R}_S = \frac{1}{N}\sum_i^N \hat{R}_S(t)</script><p>The observed variance of the net source count rates $\hat{R}_S$ is:</p><script type="math/tex; mode=display">\sigma^2_{\mathrm{obs}}=\frac{1}{N-1}\sum^N_i(\hat{R}_S(t_i)-\bar{R}_S)^2</script><p>The Poisson noise expection is computed with the mean square error computed from the error bars:</p><script type="math/tex; mode=display">\bar{\sigma^2_{\mathrm{err}}} = \frac{1}{N}\sum_i^N(\hat{\sigma}(R_S)(t_i))^2</script><p>Subtracting off this expectation, we obtain the excess variance:</p><script type="math/tex; mode=display">\sigma^2_{\mathrm{XS}} = \sigma^2_{\mathrm{obs}}-\bar{\sigma^2_{\mathrm{err}}}</script><p>Normalising to the mean count rate, gives the normalised excess variance (NEV):</p><script type="math/tex; mode=display">NEV = \frac{\sigma^2_{XS}}{\bar{R}^2_{S}}</script><p>One can also define the variable fraction of the signal, $F_{\mathrm{var}}$ as :</p><script type="math/tex; mode=display">F_{\mathrm{var}} = \sqrt{NEV}</script><p>Empirical formulas for the uncertainty in the NEV and $F_{\mathrm{var}}$ estimators are:</p><script type="math/tex; mode=display">\sigma_{\mathrm{NEV}} = \sqrt{\frac{2}{N}\frac{\bar{\sigma^2_{\mathrm{err}}}}{\bar{R}^2_S}+\frac{\bar{\sigma^2_{err}}}{N}(\frac{2F_{\mathrm{var}}}{\bar{R}_S})^2}</script><p>and,</p><script type="math/tex; mode=display">\sigma(F_{\mathrm{var}}) = \frac{\sigma(\mathrm{NEV})}{2 F_{\mathrm{var}}}</script><p>Finally, the significance of the excess variance can then be defined as:</p><script type="math/tex; mode=display">\mathrm{FVAR_SIG} = F_{\mathrm{var}}/\sigma(F_{\mathrm{var}})</script><p>and,</p><script type="math/tex; mode=display">\mathrm{NEV_SIG} = \mathrm{NEV}/\sigma(\mathrm{NEV}).</script><h3 id="Bayesian-excess-variance"><a href="#Bayesian-excess-variance" class="headerlink" title="Bayesian excess variance"></a>Bayesian excess variance</h3><p>Bayesian excess variance method wants to relax the limitation of symetric, Gaussian error bars by modelling the entire data generating process. They adopt a <strong>hierarchical Bayesian model (HBM)</strong> to model this process.</p><p>For each time bins, they assume the source count rates follow a log-normal distribution as:</p><script type="math/tex; mode=display">\log R_S(t_i) \sim \mathcal{N}(\log \bar{R}_S, \sigma_{\mathrm{bexvar}})</script><p>in which $\log bar{R}_S$ and $\sigma_{\mathrm{bexvar}}$ are the hyper-parameters. They further assign a noninoformative prior for $\log \bar{R}_S$ and $\sigma_{\mathrm{bexvar}}$ as:</p><script type="math/tex; mode=display">\log \bar{R}_S\sim \mathrm{Uniform}(-5, 5)</script><p>and,</p><script type="math/tex; mode=display">\log \sigma_{\mathrm{bexvar}} \sim \mathrm{Uniform}(-2, 2)</script><p><strong>Notice</strong>: there is a problem in their method, assign uniform distribution to $\log \sigma_{\mathrm{bexvar}}$ is inapproriate, which will lead to a improper posterior distribution!!!<br>By further incorprating the background counts $B$ and the background conunt rates $R_B(t_i)$, one can derive the joint pdf of the $2N+3$ parameters. And via <strong>Monte Carlo sampling algorithms</strong>, one can determine $P(\sigma_{\mathrm{bexvar}}|D)$ numerically.<br>In this work, they define the lower $10\%$ quantile of the distribution as a conservative indicator of the magnitude of the excess variance, and term it as SCATT_LO.</p><h2 id="Their-Result"><a href="#Their-Result" class="headerlink" title="Their Result"></a>Their Result</h2><p>They further compare different methods in the mock dataset and determine appropriate threshold for each indicator.<br>They conclude that each method has strengths in detecting certain types of variability. For flares, amplitude maximum deviation and Bayesian excess variance perform best. For intrinsic log-normal variability, the Bayesian excess variance performs best. For observing patterns yielding only few light curve data points, amplitude maximum deviation and Bayesian blocks are recommended in large surveys because they are quick and efficient, while Bayesian excess variance method cost more computational resources but will get better results.<br>They finally recommend the Bayesian excess variance and amplitude maximum deviation methods are suitable for the detection of variable sources in <em>eROSITA</em>, for the significance thresholds please check the paper.</p><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p><a href="https://arxiv.org/abs/2106.14529"><em>Systematic evaluation of variability detection methods for eROSITA, Johannes Buchner et al. 2021</em></a></p>]]></content>
      
      
      <categories>
          
          <category> Paper Reading </category>
          
      </categories>
      
      
        <tags>
            
            <tag> GSoC </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>The Rejection Method</title>
      <link href="/2022/04/09/RejectionSampling/"/>
      <url>/2022/04/09/RejectionSampling/</url>
      
        <content type="html"><![CDATA[<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>In <a href="https://zechangsun.github.io/2022/04/05/How-to-generate-simple-random-variables/">How to generate simple random variables?</a>, we show how to directly transform a uniform distributed random variable $U$ to an arbitrary random variable $X$ with cumulative density function $F(x)$. However, in real application, the inversion procedure will be very difficult if one wants to generate random variables with complicate cdf, e.g. when $F^{-1}(x)$ doesn’t have closed form. In that case, one can use the <em>rejection method</em> proposed by von Neumann in 1951:</p><p><strong> <em>Rejection sampling</em> [von Neumann (1951)] </strong>:</p><p>Suppose $l(\mathbf{x})=c\pi(\mathbf{x})$ is computable, where $\pi$ is a probability distribution function or density function and $c$ is unknown. If we can find a sampling distribution (or trial distribution) $g(\mathbf{x})$ and “covering constant” $M$ so that the envelope property $M g(\mathbf{x})\geq l(\mathbf{x})$ is satisfied for all $\mathbf{x}$, then we can apply the following procedure:</p><ul><li><p>Draw a sample $\mathbf{x}$ from $g()$ and compute the ratio</p><script type="math/tex; mode=display">r = \frac{l(\mathbf{x})}{M g(\mathbf{x})}</script></li><li><p>Flip a coin with success probability $r$;</p><ul><li>if the head turns up, we accept and return the $\mathbf{x}$;</li><li>otherwise, we reject the $\mathbf{x}$ and go back to (a).<br>Then the accepted sample follows the target distribution $\pi$.</li></ul></li></ul><h2 id="Proof"><a href="#Proof" class="headerlink" title="Proof"></a>Proof</h2><p>Let $I$ be the indicator function so that $I=1$ if the sample $\mathbf{X}$ drawn from $g()$ is accepted, and $I=0$, otherwise. Then, we can find that:</p><script type="math/tex; mode=display">P(I=1)=\int P(I=1|\mathbf{X}=\mathbf{x})g(\mathbf{x})\mathrm{d}\mathbf{x} = \int \frac{c\pi(\mathbf{x})}{Mg(\mathbf{x})}g(\mathbf{x})\mathrm{d}\mathbf{x} = \frac{c}{M}</script><p>Hence,</p><script type="math/tex; mode=display">P(\mathbf{x}|I=1) = \frac{P(\mathbf{x}, I=1)}{P(I=1)} = \frac{c\pi(\mathbf{x})}{Mg(\mathbf{x})}g(\mathbf{x})\cdot \frac{M}{c} = \pi (\mathbf{x})</script><p>The expected number of “operations” for obtaining one accepted sample is $1/P(I=1)$, which is proportional to $M$.</p><h2 id="Example"><a href="#Example" class="headerlink" title="Example"></a>Example</h2><p>We will now use the <em>rejection sampling</em> method to draw random numbers from a <strong>truncated Gaussian distribution</strong>, whose pdf $\pi(x)\propto\phi(x)I_{\{x&gt;c\}}$, where $\phi(x)$ is the standard normal density function and $I$ is the indicator function.<br>The simplest sampling strategy is that we can continue to generate random samples from a standard Gaussian distribution until a sample satisfying $X&gt;c$ is obtained. In that case, the probability we accept the sample is $\int \phi(x)\mathrm{d}x$, if $c&gt;0$, this probability can be very low which means that this sampling strategy is very inefficient. An alternative strategy is to use the <em>rejections sampling</em> method. We take the trial distribution as an exponential distribution with density function $\lambda \exp(-\lambda x)$, and then we need to determine a constant $b$ to make sure</p><script type="math/tex; mode=display">\frac{1}{b}\cdot\frac{\phi(x+c)}{1-\Phi(c)}\cdot\frac{1}{\lambda \exp(-\lambda x)}<1</script><p>for any $x&gt;0$.<br>This leads to</p><script type="math/tex; mode=display">b=\frac{\exp\{(\lambda^2-2\lambda c)/2\}}{\sqrt{2\pi}\lambda (1-\Phi(c))}</script><p>To achieve the largest sampling efficiency, one need to find the smallest value of $b$, we can do this by calculating:</p><script type="math/tex; mode=display">\frac{db}{d\lambda} = 0,</script><p>which leads to</p><script type="math/tex; mode=display">\lambda_{\mathrm{best}} = (c+\sqrt{c^2+4})/2.</script><p>We can verify above discussion as:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># draw sample by rejection method</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">simple_rejection</span>(<span class="params">c=<span class="number">1</span>, Num=<span class="number">10000</span></span>):</span><br><span class="line">    ans, N = [], <span class="number">0</span></span><br><span class="line">    <span class="keyword">while</span> <span class="built_in">len</span>(ans)&lt;Num:</span><br><span class="line">        N += <span class="number">1</span></span><br><span class="line">        r = np.random.randn()</span><br><span class="line">        <span class="keyword">if</span> r&gt;c:</span><br><span class="line">            ans += [r]</span><br><span class="line">    <span class="keyword">return</span> ans, num/N</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">exp_rejection</span>(<span class="params">c=<span class="number">1</span>, Num=<span class="number">10000</span>, lam0=<span class="literal">None</span></span>):</span><br><span class="line">    ans, N = [], <span class="number">0</span></span><br><span class="line">    <span class="keyword">if</span> lam0 <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        lam0 = <span class="number">0.5</span>*(c+(c*c+<span class="number">4</span>)**<span class="number">0.5</span>)</span><br><span class="line">    scale = <span class="number">1.</span>/lam0</span><br><span class="line">    b = np.exp((lam0**<span class="number">2</span>-<span class="number">2</span>*lam0*c)/<span class="number">2.</span>)/(<span class="number">2</span>*np.pi)**<span class="number">0.5</span>/lam0</span><br><span class="line">    <span class="keyword">while</span> <span class="built_in">len</span>(ans)&lt;Num:</span><br><span class="line">        N += <span class="number">1</span></span><br><span class="line">        x = np.random.exponential(scale)</span><br><span class="line">        lx = <span class="number">1.</span>/(<span class="number">2</span>*np.pi)**<span class="number">0.5</span>*np.exp(-<span class="number">0.5</span>*(x+c)*(x+c))</span><br><span class="line">        gx = lam0*np.exp(-<span class="number">1.</span>*lam0*x)</span><br><span class="line">        r = lx/(gx*b)</span><br><span class="line">        <span class="keyword">if</span> np.random.binomial(<span class="number">1</span>, r) == <span class="number">1</span>:</span><br><span class="line">            ans += [x+c]</span><br><span class="line">    <span class="keyword">return</span> ans, num/N</span><br><span class="line"></span><br><span class="line">num = <span class="number">100000</span></span><br><span class="line">c = <span class="number">1.</span></span><br><span class="line">simple_ans, simple_eff = simple_rejection(c=c, Num=num)</span><br><span class="line">exp_ans, exp_eff = exp_rejection(c=c, Num=num)</span><br><span class="line">exp_ans_, exp_eff_ = exp_rejection(c=c, Num=num, lam0=<span class="number">1.</span>)</span><br><span class="line"></span><br><span class="line">sns.set_style(<span class="string">&quot;white&quot;</span>)</span><br><span class="line">_ = plt.figure(figsize=(<span class="number">4</span>, <span class="number">3</span>), dpi=<span class="number">100</span>)</span><br><span class="line">_ = plt.hist(simple_ans, bins=<span class="number">100</span>, histtype=<span class="string">&#x27;step&#x27;</span>, density=<span class="literal">True</span>, ec=<span class="string">&#x27;red&#x27;</span>, fill=<span class="literal">True</span>, color=<span class="string">&#x27;salmon&#x27;</span>, linewidth=<span class="number">2.</span>, label=<span class="string">&#x27;Simple Rjection, Efficiency: %.2f&#x27;</span>%simple_eff, alpha=<span class="number">0.5</span>)</span><br><span class="line">_ = plt.hist(exp_ans, bins=<span class="number">100</span>, histtype=<span class="string">&#x27;step&#x27;</span>, density=<span class="literal">True</span>, ec=<span class="string">&#x27;blue&#x27;</span>, fill=<span class="literal">True</span>, color=<span class="string">&#x27;mediumblue&#x27;</span>, linewidth=<span class="number">2.</span>, label=<span class="string">&#x27;Exp Rejection (best c), Efficiency: %.2f&#x27;</span>%exp_eff, alpha=<span class="number">0.5</span>)</span><br><span class="line">_ = plt.hist(exp_ans_, bins=<span class="number">100</span>, histtype=<span class="string">&#x27;step&#x27;</span>, density=<span class="literal">True</span>, ec=<span class="string">&#x27;green&#x27;</span>, fill=<span class="literal">True</span>, color=<span class="string">&#x27;forestgreen&#x27;</span>, linewidth=<span class="number">2.</span>, label=<span class="string">&#x27;Exp Rejection (c=1), Efficiency: %.2f&#x27;</span>%exp_eff_, alpha=<span class="number">0.5</span>)</span><br><span class="line">_ = plt.xlim((<span class="number">1</span>, <span class="number">4</span>))</span><br><span class="line">_ = plt.xlabel(<span class="string">&quot;Sample&quot;</span>, fontdict=&#123;<span class="string">&#x27;fontsize&#x27;</span>: <span class="number">10</span>, <span class="string">&#x27;family&#x27;</span>: <span class="string">&#x27;Times New Roman&#x27;</span>&#125;)</span><br><span class="line">_ = plt.ylabel(<span class="string">&quot;Density&quot;</span>, fontdict=&#123;<span class="string">&#x27;fontsize&#x27;</span>: <span class="number">10</span>, <span class="string">&#x27;family&#x27;</span>: <span class="string">&#x27;Times New Roman&#x27;</span>&#125;)</span><br><span class="line">_ = plt.legend(loc=<span class="string">&#x27;best&#x27;</span>, fontsize=<span class="string">&#x27;9&#x27;</span>)</span><br><span class="line">leg = plt.gca().get_legend()</span><br><span class="line">ltext = leg.get_texts()</span><br><span class="line">_ = plt.setp(ltext, family=<span class="string">&#x27;Times New Roman&#x27;</span>)</span><br><span class="line">_ = plt.tick_params(axis=<span class="string">&#x27;x&#x27;</span>, which=<span class="string">&#x27;both&#x27;</span>, top=<span class="literal">False</span>, bottom=<span class="literal">True</span>)</span><br><span class="line">_ = plt.tick_params(axis=<span class="string">&#x27;y&#x27;</span>, which=<span class="string">&#x27;both&#x27;</span>, left=<span class="literal">True</span>, right=<span class="literal">False</span>, direction=<span class="string">&#x27;out&#x27;</span>)</span><br><span class="line">_ = plt.savefig(<span class="string">&#x27;tr-Gaussian.png&#x27;</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure><br>You can check the result here:</p><p><img src="https://raw.githubusercontent.com/ZechangSun/image-hosting/main/image/tr-Gaussian.webp" width="500px"></p><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul><li><em>Monte Carlo Strategies in Scientific Computing</em>, Jun S. Liu, 2001</li></ul>]]></content>
      
      
      <categories>
          
          <category> Reading Notes </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Monte Carlo Strategies in Scientific Computing </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>How to generate simple random variables ?</title>
      <link href="/2022/04/05/How-to-generate-simple-random-variables/"/>
      <url>/2022/04/05/How-to-generate-simple-random-variables/</url>
      
        <content type="html"><![CDATA[<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>Gernerating random numbers on a computer is the simplest but most basic problem in Monte Carlo sampling.<br>In this article, we will introduce how to transform random numbers from a uniform distribution to a general probability distribution.</p><h2 id="Theorm"><a href="#Theorm" class="headerlink" title="Theorm"></a>Theorm</h2><p>Suppose $F$ is a one-dimensional cumulative distribution function (cdf) and $U\sim uniform [0, 1]$. Then, $X=F^{-1}(U)$ has the distribution $F$.<br>To prove above theorm, let $F_X(x)$ be the cdf of $X$ and then:</p><script type="math/tex; mode=display">F_X(x) = \int_{-\infty}^{x} f(x)\quad dx</script><p>Here, $f(x)\cdot dx = 1\cdot du$, so</p><script type="math/tex; mode=display">F_X(x) = \int_{0}^{F(x)} 1\quad du = F(x)</script><p>So, we can sample random numbers from a simple probability distribution with cdf $F$ by transforming random numbers drawn from $Uniform[0, 1]$ using above equation.</p><h2 id="Example"><a href="#Example" class="headerlink" title="Example"></a>Example</h2><p>Here is a <code>python</code> example showing how to draw samples from a expontial distribution with density function $f(x)=\exp(-x)$, $(x&gt;1)$.<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment"># draw samples from Unifrom [0, 1]</span></span><br><span class="line">r = np.random.rand(<span class="number">10000</span>)</span><br><span class="line"><span class="comment"># transformation</span></span><br><span class="line">x = -<span class="number">1.</span> * np.log(<span class="number">1.</span> - r)</span><br><span class="line">xx = np.arange(<span class="number">0</span>, <span class="number">4</span>, <span class="number">1e-2</span>)</span><br><span class="line">y = np.exp(-<span class="number">1.</span>*xx)</span><br><span class="line"><span class="comment"># plot</span></span><br><span class="line">sns.set_style(<span class="string">&quot;white&quot;</span>)</span><br><span class="line">_ = plt.figure(figsize=(<span class="number">4</span>, <span class="number">3</span>), dpi=<span class="number">100</span>)</span><br><span class="line">_ = plt.hist(x, bins=<span class="number">100</span>, histtype=<span class="string">&#x27;step&#x27;</span>, density=<span class="literal">True</span>, ec=<span class="string">&#x27;red&#x27;</span>, fill=<span class="literal">True</span>, color=<span class="string">&#x27;salmon&#x27;</span>, linewidth=<span class="number">2.</span>, label=<span class="string">&#x27;Sample Distribution&#x27;</span>)</span><br><span class="line">_ = plt.plot(xx, y, linewidth=<span class="number">2.</span>, color=<span class="string">&#x27;blue&#x27;</span>, label=<span class="string">&#x27;Ground Truth Density Function&#x27;</span>)</span><br><span class="line">_ = plt.xlim((<span class="number">0</span>, <span class="number">4</span>))</span><br><span class="line">_ = plt.xlabel(<span class="string">&quot;Sample&quot;</span>, fontdict=&#123;<span class="string">&#x27;fontsize&#x27;</span>: <span class="number">10</span>, <span class="string">&#x27;family&#x27;</span>: <span class="string">&#x27;Times New Roman&#x27;</span>&#125;)</span><br><span class="line">_ = plt.ylabel(<span class="string">&quot;Density&quot;</span>, fontdict=&#123;<span class="string">&#x27;fontsize&#x27;</span>: <span class="number">10</span>, <span class="string">&#x27;family&#x27;</span>: <span class="string">&#x27;Times New Roman&#x27;</span>&#125;)</span><br><span class="line">_ = plt.legend(loc=<span class="string">&#x27;best&#x27;</span>, fontsize=<span class="string">&#x27;10&#x27;</span>)</span><br><span class="line">leg = plt.gca().get_legend()</span><br><span class="line">ltext = leg.get_texts()</span><br><span class="line">_ = plt.setp(ltext, family=<span class="string">&#x27;Times New Roman&#x27;</span>)</span><br><span class="line">_ = plt.title(<span class="string">&quot;Example&quot;</span>, fontdict=&#123;<span class="string">&#x27;fontsize&#x27;</span>: <span class="number">10</span>, <span class="string">&#x27;family&#x27;</span>: <span class="string">&#x27;Times New Roman&#x27;</span>&#125;)</span><br><span class="line">_ = plt.tick_params(axis=<span class="string">&#x27;x&#x27;</span>, which=<span class="string">&#x27;both&#x27;</span>, top=<span class="literal">False</span>, bottom=<span class="literal">True</span>)</span><br><span class="line">_ = plt.tick_params(axis=<span class="string">&#x27;y&#x27;</span>, which=<span class="string">&#x27;both&#x27;</span>, left=<span class="literal">True</span>, right=<span class="literal">False</span>, direction=<span class="string">&#x27;out&#x27;</span>)</span><br><span class="line">_ = plt.savefig(<span class="string">&#x27;example.png&#x27;</span>, dpi=<span class="number">100</span>)</span><br></pre></td></tr></table></figure><br>The result is:<br><img src="https://raw.githubusercontent.com/ZechangSun/image-hosting/main/image/how-to-generate-random-variables.webp" width="500px"></p><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul><li><em>Monte Carlo Strategies in Scientific Computing</em>, Jun S. Liu, 2001</li></ul>]]></content>
      
      
      <categories>
          
          <category> Reading Notes </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Monte Carlo Strategies in Scientific Computing </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hello World</title>
      <link href="/2022/03/31/hello-world/"/>
      <url>/2022/03/31/hello-world/</url>
      
        <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
      
      
      
    </entry>
    
    
  
  
</search>
